{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re, collections\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "dataframe = pd.read_csv('training_set_rel3.tsv', encoding = 'latin-1',sep='\\t')\n",
    "dataframe = dataframe[['essay_id','essay_set','essay','domain1_score']]\n",
    "dataframe.head()\n",
    "dataframe.query('essay_set==4').domain1_score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalScore(essay):\n",
    "    normal = []\n",
    "    for index,row in essay.iterrows():\n",
    "        essay_set = row['essay_set']\n",
    "        score = row['domain1_score']\n",
    "        if essay_set==1:\n",
    "            normal.append(score//7)\n",
    "        if essay_set==2:\n",
    "            normal.append(score//4)\n",
    "        if essay_set==3:\n",
    "            normal.append(score//2)\n",
    "        if essay_set==4:\n",
    "            normal.append(score//2)\n",
    "        if essay_set==5:\n",
    "            normal.append(score)\n",
    "        if essay_set==6:\n",
    "            normal.append(score)\n",
    "        if essay_set==7:\n",
    "            normal.append(score//13)\n",
    "        if essay_set==8:\n",
    "            normal.append(score//31)\n",
    "    return essay.assign(normal_score=normal)\n",
    "dataframe = normalScore(dataframe)\n",
    "dataframe.head()\n",
    "dataframe.normal_score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6703"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataframe.normal_score==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3205"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataframe.normal_score==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import enchant \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>normal_score</th>\n",
       "      <th>misspelt</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8884</th>\n",
       "      <td>14834</td>\n",
       "      <td>6</td>\n",
       "      <td>There were many obstacles that the builders fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8885</th>\n",
       "      <td>14835</td>\n",
       "      <td>6</td>\n",
       "      <td>Him from the start, there would have been many...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8886</th>\n",
       "      <td>14836</td>\n",
       "      <td>6</td>\n",
       "      <td>The builders of the Empire State Building face...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8887</th>\n",
       "      <td>14837</td>\n",
       "      <td>6</td>\n",
       "      <td>In the passage The Mooring Mast by Marcia Amid...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>14838</td>\n",
       "      <td>6</td>\n",
       "      <td>The builders of the Empire State Building face...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "8884     14834          6  There were many obstacles that the builders fa...   \n",
       "8885     14835          6  Him from the start, there would have been many...   \n",
       "8886     14836          6  The builders of the Empire State Building face...   \n",
       "8887     14837          6  In the passage The Mooring Mast by Marcia Amid...   \n",
       "8888     14838          6  The builders of the Empire State Building face...   \n",
       "\n",
       "      domain1_score  normal_score  misspelt  correct  \n",
       "8884              2             2         6      116  \n",
       "8885              3             3        11      169  \n",
       "8886              4             4         3      163  \n",
       "8887              1             1        18      175  \n",
       "8888              3             3         4      158  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "essays = dataframe\n",
    "\n",
    "# ----------- Isolate essays from the 6th set ------------ #\n",
    "essays = essays[(essays['essay_set'] == 6)]\n",
    "essays.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# load the enchant dictionary\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "def getCorrectAndIncorrectSpelling(seriesOfEssays):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples:\n",
    "    number of mispelled words, number of correctly spelled words \n",
    "    \"\"\"\n",
    "    _byRow = []\n",
    "\n",
    "    for essay in seriesOfEssays:\n",
    "        _individual = []\n",
    "        for word in essay.split():\n",
    "\n",
    "            _individual.append(d.check(word))\n",
    "\n",
    "        _byRow.append((_individual.count(False), _individual.count(True)))\n",
    "\n",
    "    return _byRow\n",
    "\n",
    "# ------ Use Enchant to get the number of misspelt and correctly spelt words ------ #\n",
    "spellings = getCorrectAndIncorrectSpelling(essays['essay'])\n",
    "\n",
    "# create a list from the tuples\n",
    "list1, list2 = zip(*spellings)\n",
    "\n",
    "# assign them to the dataframe\n",
    "essays = essays.assign(misspelt = list1) \n",
    "essays = essays.assign(correct = list2) \n",
    "essays.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Number of words in essay\n",
    "def get_essay_lengths(regExTokenizer, df):\n",
    "    \"\"\"\n",
    "    Function that gets the number of words in an\n",
    "    essay, not including punctuation and assigns them to the df\n",
    "    \"\"\"\n",
    "    length = []\n",
    "    for essay in df['essay']:\n",
    "        length.append(len(regExTokenizer.tokenize(essay)))\n",
    "        \n",
    "    return df.assign(length = length) \n",
    "\n",
    "essays = get_essay_lengths(tokenizer, essays)   \n",
    "\n",
    "# lexical diversity\n",
    "def get_lexical_diversity(regExTokenizer, df):\n",
    "    \"\"\"\n",
    "    Function that measures lexical diversity which is\n",
    "    The ratio of total words to unique words\n",
    "    Then assign that to the df\n",
    "    \"\"\"\n",
    "    ld = []\n",
    "    for essay in df['essay']:\n",
    "        \n",
    "        ld.append(round(len(regExTokenizer.tokenize(essay)) / float(len(set(regExTokenizer.tokenize(essay)))), 2))\n",
    "    return df.assign(lexical_diversity = ld)\n",
    "\n",
    "essays = get_lexical_diversity(tokenizer, essays)\n",
    "\n",
    "# number of sentences\n",
    "def get_number_of_sentences(df):\n",
    "    \"\"\"\n",
    "    Function that returns the number of sentences\n",
    "    in the document. Could be useful for run-on sentences\n",
    "    Assign them to the df\n",
    "    \"\"\"\n",
    "    \n",
    "    _byRow_sents = []\n",
    "    for essay in df['essay']:\n",
    "        sents = []\n",
    "        parsed_essay = nlp(essay)\n",
    "        for num, sentence in enumerate(parsed_essay.sents):\n",
    "            sents.append(sentence)\n",
    "        \n",
    "        _byRow_sents.append(len(sents))\n",
    "        \n",
    "    return df.assign(n_sentences = _byRow_sents)\n",
    "\n",
    "essays = get_number_of_sentences(essays)\n",
    "\n",
    "# Get Parts of Speech\n",
    "def get_list_of_number_of_pos(df):\n",
    "    \"\"\"\n",
    "    Function that parses the essay for each words POS\n",
    "    Returns tuples containg for now, nouns, verbs, adverbs and adjectives\n",
    "    \"\"\"\n",
    "    pos = []\n",
    "    \n",
    "    for essay in df['essay']:\n",
    "        parsed_essay = nlp(essay)\n",
    "        token_pos = [token.pos_ for token in parsed_essay]\n",
    "        \n",
    "        pos.append((token_pos.count('NOUN'), token_pos.count('VERB'), token_pos.count('ADV'), token_pos.count('ADJ')))\n",
    "        \n",
    "    return pos\n",
    "\n",
    "pos_list = get_list_of_number_of_pos(essays)\n",
    "nouns, verbs, adverbs, adjectives = zip(*pos_list)\n",
    "\n",
    "# Assign them to the dataframe\n",
    "essays = essays.assign(nouns = nouns)\n",
    "essays = essays.assign(verbs = verbs)\n",
    "essays = essays.assign(adverbs = adverbs)\n",
    "essays = essays.assign(adjectives = adjectives)\n",
    "\n",
    "\n",
    "# -------- Use Beautiful Soup to scrape for the top 500 SAT words ------- #\n",
    "maxdf = essays[essays.domain1_score == essays.domain1_score.max()].iloc[0]\n",
    "from rake_nltk import Rake\n",
    "r = Rake(max_length=1) # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "r.extract_keywords_from_text(maxdf.essay)\n",
    "ini = r.get_ranked_phrases()\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fin = set()\n",
    "for word in ini:\n",
    "    syns = wordnet.synsets(lemmatizer.lemmatize(word))\n",
    "    if len(syns) != 0:\n",
    "        syn = syns[0]\n",
    "        lemmas = syn.lemmas()\n",
    "        for lemma in lemmas:\n",
    "            fin.add(lemma.name())\n",
    "def wordnettopic(df):\n",
    "    col = []\n",
    "    for essay in df['essay']:\n",
    "        r.extract_keywords_from_text(essay)\n",
    "        ini = r.get_ranked_phrases()\n",
    "        tot = 0\n",
    "        for word in ini:\n",
    "            if lemmatizer.lemmatize(word) in fin:\n",
    "                tot+=1\n",
    "        col.append(tot)\n",
    "    return df.assign(wordnetscore = col)\n",
    "essays = wordnettopic(essays)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>normal_score</th>\n",
       "      <th>misspelt</th>\n",
       "      <th>correct</th>\n",
       "      <th>length</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>wordnetscore</th>\n",
       "      <th>n_top500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8884</th>\n",
       "      <td>14834</td>\n",
       "      <td>6</td>\n",
       "      <td>There were many obstacles that the builders fa...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>116</td>\n",
       "      <td>123</td>\n",
       "      <td>1.37</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8885</th>\n",
       "      <td>14835</td>\n",
       "      <td>6</td>\n",
       "      <td>Him from the start, there would have been many...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>169</td>\n",
       "      <td>180</td>\n",
       "      <td>1.55</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8886</th>\n",
       "      <td>14836</td>\n",
       "      <td>6</td>\n",
       "      <td>The builders of the Empire State Building face...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>163</td>\n",
       "      <td>169</td>\n",
       "      <td>1.62</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8887</th>\n",
       "      <td>14837</td>\n",
       "      <td>6</td>\n",
       "      <td>In the passage The Mooring Mast by Marcia Amid...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>175</td>\n",
       "      <td>199</td>\n",
       "      <td>1.69</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>14838</td>\n",
       "      <td>6</td>\n",
       "      <td>The builders of the Empire State Building face...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "      <td>162</td>\n",
       "      <td>1.74</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id  essay_set                                              essay  \\\n",
       "8884     14834          6  There were many obstacles that the builders fa...   \n",
       "8885     14835          6  Him from the start, there would have been many...   \n",
       "8886     14836          6  The builders of the Empire State Building face...   \n",
       "8887     14837          6  In the passage The Mooring Mast by Marcia Amid...   \n",
       "8888     14838          6  The builders of the Empire State Building face...   \n",
       "\n",
       "      domain1_score  normal_score  misspelt  correct  length  \\\n",
       "8884              2             2         6      116     123   \n",
       "8885              3             3        11      169     180   \n",
       "8886              4             4         3      163     169   \n",
       "8887              1             1        18      175     199   \n",
       "8888              3             3         4      158     162   \n",
       "\n",
       "      lexical_diversity  n_sentences  nouns  verbs  adverbs  adjectives  \\\n",
       "8884               1.37            9     28     14        8          10   \n",
       "8885               1.55           10     41     23       12          14   \n",
       "8886               1.62            9     39     23       10           9   \n",
       "8887               1.69           11     35     26        7           4   \n",
       "8888               1.74           11     35     26        8          15   \n",
       "\n",
       "      wordnetscore  n_top500  \n",
       "8884             3         0  \n",
       "8885             6         0  \n",
       "8886            24         0  \n",
       "8887             4         0  \n",
       "8888             8         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5620257492261873"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.tail()\n",
    "essays['n_sentences'].corr(essays['domain1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelt</th>\n",
       "      <th>correct</th>\n",
       "      <th>length</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>wordnetscore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14834</th>\n",
       "      <td>6</td>\n",
       "      <td>116</td>\n",
       "      <td>123</td>\n",
       "      <td>1.37</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14835</th>\n",
       "      <td>11</td>\n",
       "      <td>169</td>\n",
       "      <td>180</td>\n",
       "      <td>1.55</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14836</th>\n",
       "      <td>3</td>\n",
       "      <td>163</td>\n",
       "      <td>169</td>\n",
       "      <td>1.62</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14837</th>\n",
       "      <td>18</td>\n",
       "      <td>175</td>\n",
       "      <td>199</td>\n",
       "      <td>1.69</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14838</th>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "      <td>162</td>\n",
       "      <td>1.74</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          misspelt  correct  length  lexical_diversity  n_sentences  nouns  \\\n",
       "essay_id                                                                     \n",
       "14834            6      116     123               1.37            9     28   \n",
       "14835           11      169     180               1.55           10     41   \n",
       "14836            3      163     169               1.62            9     39   \n",
       "14837           18      175     199               1.69           11     35   \n",
       "14838            4      158     162               1.74           11     35   \n",
       "\n",
       "          verbs  adverbs  adjectives  wordnetscore  \n",
       "essay_id                                            \n",
       "14834        14        8          10             3  \n",
       "14835        23       12          14             6  \n",
       "14836        23       10           9            24  \n",
       "14837        26        7           4             4  \n",
       "14838        26        8          15             8  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.set_index('essay_id',inplace=True, drop=True)\n",
    "X = essays.drop(['domain1_score','essay','essay_set','normal_score','n_top500'], axis=1)\n",
    "y = essays['normal_score']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3876073238853832"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)\n",
    "dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred= dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = cohen_kappa_score(y_pred, y_test)\n",
    "accuracy_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.429310766863657"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_gini = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_gini.fit(X_train, y_train)\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred= dt_gini.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_gini = cohen_kappa_score(y_pred, y_test)\n",
    "accuracy_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59722222, 0.58680556, 0.58680556, 0.58680556, 0.56597222])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=2)\n",
    "\n",
    "# split data set into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)\n",
    "\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "Accuracy_CV_scores = cross_val_score(dt_entropy, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1) \n",
    "\n",
    "Accuracy_CV_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.515\n",
      "K Nearest Neighbours : 0.594\n",
      "Classification Tree : 0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED=3\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNN(n_neighbors=5)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=SEED)\n",
    "\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=26)\n",
    "\n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_pred, y_test) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# split data set into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=29)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=4, min_samples_leaf=0.016, random_state=4)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=12)\n",
    "\n",
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_pred, y_test)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.61\n",
      "OOB accuracy of bc: 0.60\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=4, min_samples_leaf=0.016, random_state=4)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1, random_state=13)\n",
    "\n",
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_pred, y_test)\n",
    "\n",
    "# Extract the OOB accuracy from bc\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) \n",
    "\n",
    "print('OOB accuracy of bc: {:.2f}'.format(oob_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestClassifier(criterion='gini', random_state=2)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 2,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': ['gini', 'entropy'],\n",
       " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
       " 'max_features': ['auto', 'sqrt'],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# criterion for information gain\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'criterion': criterion,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   48.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=2),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': None,\n",
       " 'criterion': 'entropy',\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "[[  2   0   0   2   0]\n",
      " [  2   5  26   2   0]\n",
      " [  0   1  34  37   1]\n",
      " [  0   0  18 128  25]\n",
      " [  0   0   0  34  43]]\n",
      "\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50         4\n",
      "           1       0.83      0.14      0.24        35\n",
      "           2       0.44      0.47      0.45        73\n",
      "           3       0.63      0.75      0.68       171\n",
      "           4       0.62      0.56      0.59        77\n",
      "\n",
      "    accuracy                           0.59       360\n",
      "   macro avg       0.60      0.48      0.49       360\n",
      "weighted avg       0.61      0.59      0.57       360\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "rf_random = RandomForestClassifier(criterion='entropy', \n",
    "                            n_estimators=1000, \n",
    "                            min_samples_leaf=4, \n",
    "                            min_samples_split=2, \n",
    "                            max_features='auto',\n",
    "                            max_depth=None,\n",
    "                            bootstrap=True,\n",
    "                            random_state=2)\n",
    "\n",
    "# Fit rf to the training set    \n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = rf_random.predict(X_test)\n",
    "\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = pd.Series(data=rf_random.feature_importances_, index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c2a7243ce08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
